shared:
  task: "REGRESSION"

  # project configurations for experiment tracking - wandb
  project: "learning-agency-lab-automated-essay-scoring-2"
  entity: "gaohn-teaching-mlops"
  name: null
  job_type: "train"
  tags: null
  group: null
  notes: null
  mode: null

  # data configurations
  ## filepaths
  train_filepath: "lal/data/train.csv"
  valid_filepath: null
  pretraining_data_filepath: null
  external_data_filepath: "lal/data/persuade_2.0_human_scores_demo_id_github.csv"
  predicted_prompt_filepath: "lal/data/predicted_prompt.csv"
  train_topic_filepath: "lal/data/train_topic.csv"
  topics_map_filepath: "lal/conf/topics_map.json"

  system_prompt: "Please read the following essay and assign a score of 1,2,3,4,5,6 where 6 is the best. Output only a single number with no explanation.\n\n"
  essay_id: "essay_id"
  full_text: "full_text"
  score: "score"
  label: "label"
  description: "description"
  num_labels: 1 # CHANGE

  # split/resample strategy
  resample_strategy: "StratifiedKFold" # "StratifiedKFold" "StratifiedGroupKFold" # CHANGE
  resample_params:
    n_splits: 5 # CHANGE
    shuffle: true
    random_state: 20230310
  # resample_strategy: "StratifiedGroupKFold" # "StratifiedKFold" "StratifiedGroupKFold" # CHANGE
  # resample_params:
  #   n_splits: 7 # CHANGE
  #   shuffle: true
  #   random_state: 42
  group_by: "prompt_name" # "prompt_name" # CHANGE
  stratify_by: "score" # null # CHANGE
  fold_column: "fold"
  target_column: "label"
  fold: 2
  debug_samples: 128

  # from_pretrained configurations
  padding_side: "right"

  # kwargs to tokenizer(...)
  max_length: 1024
  truncation: true
  return_tensors: null
  add_special_tokens: true
  padding: null

  return_tokenized_text: true

  # peft
  task_type: "SEQ_CLS"
  inference_mode: false
  r: 32
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj"]
  bias: "none"
  modules_to_save: ["score"]

  # **kwargs to model(...)
  force_download: false
  device_map: "auto"
  torch_dtype: "float32"
  load_in_4bit: false
  output_hidden_states: true
  output_attentions: false

  model_type: SubclassedDebertaV2ForSequenceClassification

  # pooler
  pooler_type: attention # CHANGE
  pooler_config: {}

  # criterion
  criterion: huber # CHANGE
  criterion_config: {}

  pretrained_model_name_or_path: "microsoft/deberta-v3-large"
  load_backbone_only: false
  cache_dir: "/root/.cache/huggingface"
  target_artifacts_dir: "/artifacts"
  verbose: true
  show_model_summary: true
  dry_run: true

  load_from: null
  inference: false
  use_lora: false
  num_layers_to_remove: null
  reinitialize_n_layers_of_backbone: 0
  enable_gradient_checkpointing: false

  freeze_these_layers_indices: null
  freeze_embeddings: false

  # seed
  seed: 20230310
  seed_torch: true
  set_torch_deterministic: false

  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-6
  bf16: false
  data_seed: 20230310
  dataloader_num_workers: 0
  ddp_find_unused_parameters: null
  disable_tqdm: false
  do_eval: true
  do_predict: false
  do_train: true
  eval_strategy: "steps"
  eval_steps: null
  fp16: false
  full_determinism: false
  gradient_accumulation_steps: 1 # CHANGE
  gradient_checkpointing: false
  greater_is_better: true # CHANGE
  include_tokens_per_second: false
  label_smoothing_factor: 0.0
  learning_rate: 0.00001 # CHANGE
  load_best_model_at_end: true
  logging_dir: null
  logging_first_step: true
  logging_steps: null
  logging_strategy: "steps"
  lr_scheduler_kwargs: {}
  lr_scheduler_type: "cosine" # CHANGE
  max_grad_norm: 10 # CHANGE
  max_steps: null
  metric_for_best_model: "eval_qwk" # CHANGE
  num_train_epochs: 4 # CHANGE
  optim: "adamw_torch"
  output_dir: null
  overwrite_output_dir: true
  per_device_eval_batch_size: 8 # CHANGE
  per_device_train_batch_size: 8 # CHANGE
  prediction_loss_only: false
  report_to: "wandb"
  restore_callback_states_from_checkpoint: false
  resume_from_checkpoint: null # CHANGE
  save_only_model: false
  save_on_each_node: false
  save_safetensors: true
  save_steps: null
  save_strategy: "steps"
  save_total_limit: 3
  warmup_ratio: 0 # CHANGE
  warmup_steps: 0
  weight_decay: 0.01

  base_learning_rate: 0.00001 # CHANGE
  desired_effective_batch_size: 8 # CHANGE
  enable_mixed_precision: true
  scheduler_specific_kwargs: {}
  very_custom_optimizer_group: false
  layer_wise_learning_rate_decay: null
