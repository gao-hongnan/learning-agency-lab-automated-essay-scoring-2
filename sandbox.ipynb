{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gaohn/gaohn/learning-agency-lab-automated-essay-scoring-2'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DebertaV2ForSequenceClassification\n",
    "from torch import nn\n",
    "from typing import List, Dict, Tuple, Iterator, Literal\n",
    "from rich.pretty import pprint\n",
    "from torch.optim import AdamW\n",
    "from omnivault.utils.torch_utils.model_utils import get_named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL: nn.Module = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n",
    "\n",
    "\n",
    "def get_parameter_names(model, forbidden_layer_types):\n",
    "    \"\"\"\n",
    "    Returns the names of the model parameters that are not inside a forbidden layer.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for name, child in model.named_children():\n",
    "        result += [\n",
    "            f\"{name}.{n}\"\n",
    "            for n in get_parameter_names(child, forbidden_layer_types)\n",
    "            if not isinstance(child, tuple(forbidden_layer_types))\n",
    "        ]\n",
    "    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n",
    "    result += list(model._parameters.keys())\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_decay_parameter_names(model) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all parameter names that weight decay will be applied to\n",
    "\n",
    "    Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still\n",
    "    apply to those modules since this function only filter out instance of nn.LayerNorm\n",
    "    \"\"\"\n",
    "    decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    return decay_parameters\n",
    "\n",
    "\n",
    "decay_parameters = get_decay_parameter_names(BASE_MODEL)\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in BASE_MODEL.named_parameters() if (n in decay_parameters and p.requires_grad)]},\n",
    "    {\"params\": [p for n, p in BASE_MODEL.named_parameters() if n not in decay_parameters and p.requires_grad]},\n",
    "]\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-6,\n",
    "    capturable=False,\n",
    "    differentiable=False,\n",
    "    maximize=False,\n",
    "    amsgrad=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the above in yields same results as if you set config in `TrainingArguments`\n",
    "for `adamw_torch` with same parameters above. The decay group is set as such in\n",
    "the huggingface's transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'EMBEDDINGS'</span>: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">generator</span><span style=\"color: #000000; text-decoration-color: #000000\"> object Module.named_parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2b34f7c40</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'BACKBONE'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;generator object Module.named_parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2bd431740</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'POOLER'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;generator object Module.named_parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2bd431640</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'HEAD'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;generator object Module.named_parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2bd431940</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'EMBEDDINGS'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;95mgenerator\u001b[0m\u001b[39m object Module.named_parameters at \u001b[0m\u001b[1;36m0x2b34f7c40\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'BACKBONE'\u001b[0m\u001b[39m: <generator object Module.named_parameters at \u001b[0m\u001b[1;36m0x2bd431740\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'POOLER'\u001b[0m\u001b[39m: <generator object Module.named_parameters at \u001b[0m\u001b[1;36m0x2bd431640\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'HEAD'\u001b[0m\u001b[39m: <generator object Module.named_parameters at \u001b[0m\u001b[1;36m0x2bd431940\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert isinstance(BASE_MODEL, DebertaV2ForSequenceClassification)\n",
    "\n",
    "\n",
    "named_parameters_grouped: Dict[\n",
    "    Literal[\"EMBEDDINGS\", \"BACKBONE\", \"POOLER\", \"HEAD\"], Iterator[Tuple[str, nn.Parameter]]\n",
    "] = {\n",
    "    \"EMBEDDINGS\": BASE_MODEL.deberta.embeddings.named_parameters(),\n",
    "    \"BACKBONE\": BASE_MODEL.deberta.encoder.named_parameters(),\n",
    "    \"POOLER\": BASE_MODEL.pooler.named_parameters(),\n",
    "    \"HEAD\": BASE_MODEL.classifier.named_parameters(),\n",
    "}\n",
    "NO_DECAY: List[str] = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]  # CHANGE AS YOU WISH\n",
    "\n",
    "pprint(named_parameters_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_and_backbone = [BASE_MODEL.deberta.embeddings] + list(BASE_MODEL.deberta.encoder.layer)\n",
    "embeddings_and_backbone.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_settings = {\n",
    "    \"EMBEDDINGS\": (0.01, 1e-5),  # (weight_decay, lr)\n",
    "    \"BACKBONE\": (0.03, 1e-4),  # (weight_decay, lr)\n",
    "    \"POOLER\": (0.0, 1e-4),  # (weight_decay, lr)\n",
    "    \"HEAD\": (0.0, 1e-4),  # (weight_decay, lr)\n",
    "}\n",
    "\n",
    "EMBEDDINGS_PARAM_GROUP = {\n",
    "    \"params\": [\n",
    "        parameter\n",
    "        for parameter_name, parameter in named_parameters_grouped[\"EMBEDDINGS\"]\n",
    "        if not any(nd in parameter_name for nd in NO_DECAY)\n",
    "    ],\n",
    "    \"lr\": group_settings[\"EMBEDDINGS\"][1],\n",
    "    \"weight_decay\": group_settings[\"EMBEDDINGS\"][0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.1034e-03, -3.6097e-04, -1.9512e-03,  ..., -5.6000e-03,\n",
       "          -6.8550e-03,  3.3997e-02],\n",
       "         [ 1.1765e-02,  1.6769e-02, -1.3268e-02,  ...,  1.4679e-02,\n",
       "          -3.7575e-03,  4.2969e-02],\n",
       "         [ 2.1271e-02,  1.5610e-02, -1.2688e-02,  ...,  1.9821e-02,\n",
       "          -1.6876e-02,  3.2959e-02],\n",
       "         ...,\n",
       "         [ 4.1008e-05, -4.1733e-03, -1.3351e-04,  ..., -1.5106e-02,\n",
       "          -5.4779e-03,  3.0121e-02],\n",
       "         [-6.1836e-03, -3.7823e-03, -6.9580e-03,  ..., -6.7368e-03,\n",
       "          -5.2834e-03,  3.0716e-02],\n",
       "         [-5.6953e-03, -4.5681e-04,  1.1883e-03,  ..., -6.3858e-03,\n",
       "          -8.9340e-03,  3.2684e-02]], requires_grad=True)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDINGS_PARAM_GROUP[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_parameters(\n",
    "    named_parameters: Iterator[Tuple[str, nn.Parameter]],\n",
    "    weight_decay: float,\n",
    "    lr: float\n",
    ") -> Dict[str, List[Dict[str, object]]]:\n",
    "    decay, no_decay = [], []\n",
    "    for parameter_name, parameter in named_parameters:\n",
    "        if any(black in parameter_name for black in NO_DECAY):\n",
    "            no_decay.append({\"params\": parameter, \"weight_decay\": 0.0, \"lr\": lr})\n",
    "        elif \"weight\" in parameter_name and \"bias\" not in parameter_name:\n",
    "            decay.append({\"params\": parameter, \"weight_decay\": weight_decay, \"lr\": lr})\n",
    "        else:\n",
    "            no_decay.append({\"params\": parameter, \"weight_decay\": 0.0, \"lr\": lr})\n",
    "    return {\"decay\": decay, \"no_decay\": no_decay}\n",
    "\n",
    "def set_parameters_by_group(group_settings: Dict[str, Tuple[float, float]]) -> List[Dict[str, object]]:\n",
    "    param_groups = []\n",
    "    named_params = get_named_parameters()\n",
    "    for group_name, (weight_decay, lr) in group_settings.items():\n",
    "        group_params = categorize_parameters(named_params[group_name], weight_decay, lr)\n",
    "        param_groups.extend(group_params[\"decay\"])\n",
    "        param_groups.extend(group_params[\"no_decay\"])\n",
    "    return param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta.embeddings.word_embeddings.weight True\n",
      "deberta.embeddings.LayerNorm.weight True\n",
      "deberta.embeddings.LayerNorm.bias True\n",
      "deberta.encoder.layer.0.attention.self.query_proj.weight True\n",
      "deberta.encoder.layer.0.attention.self.query_proj.bias True\n",
      "deberta.encoder.layer.0.attention.self.key_proj.weight True\n",
      "deberta.encoder.layer.0.attention.self.key_proj.bias True\n",
      "deberta.encoder.layer.0.attention.self.value_proj.weight True\n",
      "deberta.encoder.layer.0.attention.self.value_proj.bias True\n",
      "deberta.encoder.layer.0.attention.output.dense.weight True\n",
      "deberta.encoder.layer.0.attention.output.dense.bias True\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.0.intermediate.dense.weight True\n",
      "deberta.encoder.layer.0.intermediate.dense.bias True\n",
      "deberta.encoder.layer.0.output.dense.weight True\n",
      "deberta.encoder.layer.0.output.dense.bias True\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.1.attention.self.query_proj.weight True\n",
      "deberta.encoder.layer.1.attention.self.query_proj.bias True\n",
      "deberta.encoder.layer.1.attention.self.key_proj.weight True\n",
      "deberta.encoder.layer.1.attention.self.key_proj.bias True\n",
      "deberta.encoder.layer.1.attention.self.value_proj.weight True\n",
      "deberta.encoder.layer.1.attention.self.value_proj.bias True\n",
      "deberta.encoder.layer.1.attention.output.dense.weight True\n",
      "deberta.encoder.layer.1.attention.output.dense.bias True\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.1.intermediate.dense.weight True\n",
      "deberta.encoder.layer.1.intermediate.dense.bias True\n",
      "deberta.encoder.layer.1.output.dense.weight True\n",
      "deberta.encoder.layer.1.output.dense.bias True\n",
      "deberta.encoder.layer.1.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.1.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.2.attention.self.query_proj.weight True\n",
      "deberta.encoder.layer.2.attention.self.query_proj.bias True\n",
      "deberta.encoder.layer.2.attention.self.key_proj.weight True\n",
      "deberta.encoder.layer.2.attention.self.key_proj.bias True\n",
      "deberta.encoder.layer.2.attention.self.value_proj.weight True\n",
      "deberta.encoder.layer.2.attention.self.value_proj.bias True\n",
      "deberta.encoder.layer.2.attention.output.dense.weight True\n",
      "deberta.encoder.layer.2.attention.output.dense.bias True\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.2.intermediate.dense.weight True\n",
      "deberta.encoder.layer.2.intermediate.dense.bias True\n",
      "deberta.encoder.layer.2.output.dense.weight True\n",
      "deberta.encoder.layer.2.output.dense.bias True\n",
      "deberta.encoder.layer.2.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.2.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.3.attention.self.query_proj.weight True\n",
      "deberta.encoder.layer.3.attention.self.query_proj.bias True\n",
      "deberta.encoder.layer.3.attention.self.key_proj.weight True\n",
      "deberta.encoder.layer.3.attention.self.key_proj.bias True\n",
      "deberta.encoder.layer.3.attention.self.value_proj.weight True\n",
      "deberta.encoder.layer.3.attention.self.value_proj.bias True\n",
      "deberta.encoder.layer.3.attention.output.dense.weight True\n",
      "deberta.encoder.layer.3.attention.output.dense.bias True\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.3.intermediate.dense.weight True\n",
      "deberta.encoder.layer.3.intermediate.dense.bias True\n",
      "deberta.encoder.layer.3.output.dense.weight True\n",
      "deberta.encoder.layer.3.output.dense.bias True\n",
      "deberta.encoder.layer.3.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.3.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.4.attention.self.query_proj.weight True\n",
      "deberta.encoder.layer.4.attention.self.query_proj.bias True\n",
      "deberta.encoder.layer.4.attention.self.key_proj.weight True\n",
      "deberta.encoder.layer.4.attention.self.key_proj.bias True\n",
      "deberta.encoder.layer.4.attention.self.value_proj.weight True\n",
      "deberta.encoder.layer.4.attention.self.value_proj.bias True\n",
      "deberta.encoder.layer.4.attention.output.dense.weight True\n",
      "deberta.encoder.layer.4.attention.output.dense.bias True\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.4.intermediate.dense.weight True\n",
      "deberta.encoder.layer.4.intermediate.dense.bias True\n",
      "deberta.encoder.layer.4.output.dense.weight True\n",
      "deberta.encoder.layer.4.output.dense.bias True\n",
      "deberta.encoder.layer.4.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.4.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.5.attention.self.query_proj.weight True\n",
      "deberta.encoder.layer.5.attention.self.query_proj.bias True\n",
      "deberta.encoder.layer.5.attention.self.key_proj.weight True\n",
      "deberta.encoder.layer.5.attention.self.key_proj.bias True\n",
      "deberta.encoder.layer.5.attention.self.value_proj.weight True\n",
      "deberta.encoder.layer.5.attention.self.value_proj.bias True\n",
      "deberta.encoder.layer.5.attention.output.dense.weight True\n",
      "deberta.encoder.layer.5.attention.output.dense.bias True\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "deberta.encoder.layer.5.intermediate.dense.weight True\n",
      "deberta.encoder.layer.5.intermediate.dense.bias True\n",
      "deberta.encoder.layer.5.output.dense.weight True\n",
      "deberta.encoder.layer.5.output.dense.bias True\n",
      "deberta.encoder.layer.5.output.LayerNorm.weight True\n",
      "deberta.encoder.layer.5.output.LayerNorm.bias True\n",
      "deberta.encoder.rel_embeddings.weight True\n",
      "deberta.encoder.LayerNorm.weight True\n",
      "deberta.encoder.LayerNorm.bias True\n",
      "pooler.dense.weight True\n",
      "pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "model_named_parameters = get_named_parameters(BASE_MODEL)\n",
    "for module_info in model_named_parameters:\n",
    "    for name, param in module_info.items():\n",
    "        print(name, param.requires_grad)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer_grouped_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m----> 4\u001b[0m             p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mopt_model\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m (n \u001b[38;5;129;01min\u001b[39;00m decay_parameters \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m      5\u001b[0m         ],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay,\n\u001b[1;32m      7\u001b[0m     },\n\u001b[1;32m      8\u001b[0m     {\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m             p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m opt_model\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m (n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m decay_parameters \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m     11\u001b[0m         ],\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     13\u001b[0m     },\n\u001b[1;32m     14\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt_model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "        ],\n",
    "        \"weight_decay\": self.args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
